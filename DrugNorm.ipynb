{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrugNorm \n",
    "\n",
    "author -- AR Dirkson --\n",
    "\n",
    "date -- 08-02-2019 --\n",
    " \n",
    "python version -- 3 --\n",
    "\n",
    "This script  first subsets the dictionary for the drug names that are in your corpus and then uses simple matching to replace them by the generic drug name chosen as a key in the dictionary.\n",
    "\n",
    "The CELEX_lwrd_unique is a list of all the unique lowercased words in the CELEX. Alternatives can be used but must be in list form for this script.\n",
    "\n",
    "Data input needs to be tokenized and the module only deals with lowercased words! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugNorm (): \n",
    "    \n",
    "    def __init__(self): \n",
    "        pass\n",
    "    \n",
    "    #to use this function the files need to be sorted in the same folder as the script under /obj_lex/\n",
    "    def load_obj(self, name):\n",
    "        with open('obj_lex/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    def subset_drug_normalize_dict (self, msgs):\n",
    "        drug_norm_dict = self.load_obj('drug_normalize_dict')\n",
    "        \n",
    "        #subset the dictionary for the drug names actually used in the data\n",
    "        alt_names_flat = [item for sublist in list(drug_norm_dict.values()) for item in sublist]\n",
    "        set_drug = set(alt_names_flat)\n",
    "        msgs_flat = [item for sublist in msgs for item in sublist]\n",
    "        set_msgs = set (msgs_flat)\n",
    "\n",
    "        inters_drug_msgs = set_drug.intersection(set_msgs)\n",
    "        \n",
    "        #remove all words from the drug normalization subset that are generic words in the CELEX using a set operation\n",
    "        lex_normal= self.load_obj ('celex_lwrd_unique')\n",
    "\n",
    "        lex_normal2 = list(lex_normal)\n",
    "        lex_normal_set = set(lex_normal2)\n",
    "        \n",
    "        inters_drug_msgs_remove= lex_normal_set.intersection(inters_drug_msgs)\n",
    "\n",
    "        inters_drug_msgs_new = []\n",
    "\n",
    "        for word in inters_drug_msgs: \n",
    "            if word not in inters_drug_msgs_remove: \n",
    "                    inters_drug_msgs_new.append(word)\n",
    "\n",
    "        inters_drug_msgs_new2 = []\n",
    "\n",
    "        for a, word in enumerate (inters_drug_msgs_new):  \n",
    "            if len(word) > 2: \n",
    "                inters_drug_msgs_new2.append(word)\n",
    "\n",
    "        drug_norm_dict_small = {}\n",
    "\n",
    "        for key, value in drug_norm_dict.items():\n",
    "            temp = []\n",
    "            for word in value: \n",
    "                if word in inters_drug_msgs_new2: \n",
    "                    temp.append(word)\n",
    "            drug_norm_dict_small[key] = temp\n",
    "\n",
    "        #Remove all keys with an empty list\n",
    "        list_of_kept_keys = []\n",
    "\n",
    "        for key,value in drug_norm_dict_small.items(): \n",
    "            if value != []:\n",
    "                list_of_kept_keys.append(key)\n",
    "\n",
    "        drug_norm_subdict_small = {k: drug_norm_dict_small[k] for k in (list_of_kept_keys)}\n",
    "        \n",
    "        return drug_norm_subdict_small, inters_drug_msgs_new2\n",
    "\n",
    "    #normalization \n",
    "    def drug_normalize (self, msgs): \n",
    "        drug_norm_dict, inters_drug_msgs = self.subset_drug_normalize_dict (msgs)            \n",
    "        msgs2 = []\n",
    "        total_cnt = []\n",
    "        replaced = []\n",
    "        replaced_with  = []\n",
    "        for post in msgs: \n",
    "            cnt = 0\n",
    "            for a, word in enumerate (post): \n",
    "                if word in inters_drug_msgs:\n",
    "                    for key, value in drug_norm_dict.items (): \n",
    "                        if word in value: \n",
    "                            cnt += 1 \n",
    "                            txt = word.replace (word, key)\n",
    "                            replaced.append (word)\n",
    "                            replaced_with.append (key)\n",
    "                            post[a] = txt\n",
    "            total_cnt.append (cnt)\n",
    "            msgs2.append(post)\n",
    "        return msgs2, total_cnt, replaced, replaced_with\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = ['the drug imatinib causes nausea', 'paracetamol is good for headaches', 'ibuprofen helps to relieve']\n",
    "\n",
    "msgs_tok = [word_tokenize(m) for m in msgs]\n",
    "\n",
    "msgs2, total_cnt, replaced, replaced_with = DrugNorm().drug_normalize(msgs_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'drug', 'imatinib', 'causes', 'nausea'], ['paracetamol', 'is', 'good', 'for', 'headaches'], ['ibuprofen', 'helps', 'to', 'relieve']]\n"
     ]
    }
   ],
   "source": [
    "print(msgs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
